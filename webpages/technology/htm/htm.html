<!DOCTYPE html>
<html>
<head>
	<title>Neuroowl - HTM </title>
	<link rel="stylesheet" type="text/css" href="/style.css" />
	<link rel="shortcut icon" type="image/x-icon" href="/resources/favicon.png" />
	<script type="text/javascript" src="/javascript/jquery-3.1.0.min.js"></script>
	<script type="text/javascript" src="/javascript/navbar-load.js"></script>
</head>
<body>
<div class="navbar"></div>
<div class="text">
	<h1>Hierarchical Temporal Memory</h1>

	<p>Hierarchical Temporal Memory (HTM) is a theoretical framework of the neocortex for both biological and machine intelligence under development by Numenta <a href="#1">[1]</a>.</p>
	
	<h2>Introduction</h2>
	
	<p>
	The goal of building intelligent machines is not to replicate human	behavior, nor to build a brain, nor to create machines to do what humans do.  The goal of building intelligent machines is to create machines that work on
	the same principles as the brain -- machines that are able to learn, discover, and adapt in ways that computers can't and brains can.  HTM is a	theory of the neocortex and a few related brain structures; it is not an
	attempt to model or understand every part of the human brain. <a href="#2">[2]</a>
	</p>
		
	<h2>HTM Principles</h2>

	<ul>
		<li><b>Common Algorithms in Regions</b> - Every region of the neocortex performs the same set of memory and algorithmic functions, regardless of processing vision, hearing, touch, language, etc.</li>
		<li><b>Hierarchy of Regions</b> - Neocortex organizes regions in a hierarchy</li>
		<li><b>Sparse Distributed Representations (SDRs)</b> - only a small percentage of a population of neurons are active at one time.  Symantic similarity</li>
		<li><b>Sensory Encoders</b> - Converts some type of data into an SDR to be processed by HTM algorithms</li>
		<li><b>Sensory-motor Systems</b> - we can't build systems that see and hear like humans do without incorporating movement of the eyes, body, and limbs.</li>
		<li><b>Streaming Data and Sequence Memory</b> - </li>
		<li><b>On-line Learning</b> - </li>
		<li><b>Unsupervised</b> - </li>
	</ul>
		
	<h2>Architecture</h2>

	<h3>Synapses</h3>

	Address
	Permanence

	<h3>Dendrite Segments</h3>

	Proximal
	Distal
	Apical

	<h3>Cells</h3>

	<h3>Columns</h3>

	<h3>Regions</h3>

	<h2>Algorithms</h2>

	<h3>Spatial Pooling</h3>

	<img src="/webpages/technology/htm/sp.gif" alt="sp" style="margin: 25px auto">

	<h4>Overlap</h4>

	<h4>Inhibition</h4>

	<h4>Learning</h4>

	<h3>Temporal Memory</h3>

	<img src="/webpages/technology/htm/tm.gif" alt="tm" style="margin: 25px auto">

	<h4>Bursting</h4>

	<h4>Prediction</h4>

	<h4>Learning</h4>
	


		<h3>Synaptic Learning</h3>
		<p>
			Each dendrite segment has synapses, which form the fundamental memory
			storage of the neocortex.  For a neuron to recognize a pattern of
			activity it requires a set of co-located synapses (typically 15-20)
			that connect to a subset of the cells that are active in the pattern to
			be recognized. Learning to recognize a new pattern is accomplished by
			the formation of a set of new synapses collocated on a dendritic
			segment. <a href="#3">[3]</a>
		</p>
		<p>
			The permanence value is incremented and decremented using a Hebbian-like
			rule. If the permanence value exceeds a threshold, such as 0.3, then
			the weight of the synapse is 1, if the permanence value is at or below
			the threshold then the weight of the synapse is 0. The threshold
			represents the establishment of a synapse, albeit one that could easily
			disappear. A synapse with a permanence value of 1.0 has the same effect
			as a synapse with a permanence value at threshold but is not as easily
			forgotten. Using a scalar permanence value enables on-line learning in
			the presence of noise. A previously unseen input pattern could be noise
			or it could be the start of a new trend that will repeat in the future.
			By growing new synapses, the network can start to learn a new pattern
			when it is first encountered, but only act differently after several
			presentations of the new pattern. Increasing permanence beyond the
			threshold means that patterns experienced more than others will take
			longer to forget. <a href="#3">[3]</a>
		</p>
		
		<p>
			*Note: Numenta uses floating permanence values from 0.0 to 1.0.  I
			prefer to use int8 values from 0 to 99, which is why the figures above
			display integer permanence values.
		</p>
		
		<h2>References</h2>
		<p>
			<a name="1">[1]</a>
			&nbsp; <a href="http://numenta.com/">link</a>
			&nbsp; Numenta.
		</p>
		<p>
			<a name="2">[2]</a>
			&nbsp; <a href="http://numenta.com/biological-and-machine-intelligence/">link</a>
			&nbsp; Jeff Hawkins et al. Biological and Machine Intelligence, 2016.
		</p>
		<p>
			<a name="3">[3]</a>
			&nbsp; <a href="http://journal.frontiersin.org/article/10.3389/fncir.2016.00023/full">link</a>
			&nbsp; Jeff Hawkins Subutai Ahmad. Why Neurons Have Thousands of Synapses a
			Theory of Sequence Memory in Neocortex, 2016.
		</p>
		<p>
			<a name="4">[4]</a>
			&nbsp; <a href="https://github.com/numenta/nupic.research/blob/master/docs/bibliography/htm_bibliography.pdf">link</a>
			&nbsp; Subutai Ahmad and Yuwei Chu. Annotated Bibliography for HTM Researchers, 2016.
		</p>
		<p>
			<a name="5">[5]</a>
			&nbsp; <a href="https://arxiv.org/abs/1512.05463">link</a>
			&nbsp; Yuwei Cui, Subutai Ahmad and Jeff Hawkins. Continuous Online Sequence Learning with an Unsupervised Neural Network Model, 2015.
		</p>
		<p>
			<a name="6">[6]</a>
			&nbsp; <a href="https://discourse.numenta.org/t/preliminary-details-about-new-theory-work-on-sensory-motor-inference/697">link</a>
			&nbsp; Numenta Forum. Preliminary details about new theory work on sensory-motor inference, 2016.
		</p>
		<p>WIP:</p>
		<p></p>
		<p>https://discourse.numenta.org/t/temporal-memory-handling-no-previous-winner-cells/1250</p>
		<p>https://discourse.numenta.org/t/understanding-the-phrase-layer-x-projects-to-layer-y/1192/3</p>
		<p>Page update: 2017-06-06</p>
</div>
</body>
</html>