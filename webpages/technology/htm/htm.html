<!DOCTYPE html>
<html>
<head>
	<title>Neuroowl - HTM </title>
	<link rel="stylesheet" type="text/css" href="/style.css" />
	<link rel="shortcut icon" type="image/x-icon" href="/resources/favicon.png" />
	<script type="text/javascript" src="/javascript/jquery-3.1.0.min.js"></script>
	<script type="text/javascript" src="/javascript/navbar-load.js"></script>
</head>
<body>
<div class="navbar"></div>
<div class="text">
	<h1>Hierarchical Temporal Memory</h1>

	<p>Hierarchical Temporal Memory (HTM) is a theoretical framework of the neocortex for both biological and machine intelligence under development by <a href="http://numenta.com/">Numenta</a>.</p>

	<h2>Introduction</h2>
	
	<p>
	"The goal of building intelligent machines is not to replicate human behavior, nor to build a brain, nor to create machines to do what humans do.  The goal of building intelligent machines is to create machines that work on
	the same principles as the brain -- machines that are able to learn, discover, and adapt in ways that computers can't and brains can.  HTM is a	theory of the neocortex and a few related brain structures; it is not an 
	attempt to model or understand every part of the human brain." <sup><a href="#1">[1]</a></sup>
	</p>
		
	<h2>HTM Principles</h2>

	<input class="toggle-box" id="p0" type="checkbox">
	<label for="p0"><h3>Hierarchical</h3></label>
	<div>
		<p>
		The regions of the neocortex are connected in a logical "hierarchy". Because all the regions of the neocortex perform the same basic memory operations, the detailed understanding of one neocortical region leads us to 
		understand how the rest of the neocortex works.
		</p>
	</div>

	<input class="toggle-box" id="p1" type="checkbox">
	<label for="p1"><h3>Temporal (Sequence Memory)</h3></label>
	<div>
		<p>
		The memory in the neocortex is primarily a memory of time changing, or "temporal", patterns.  The inputs and outputs of the neocortex are constantly in motion, usually changing completely several times a second.  Each 
		region of the neocortex learns a time-based model of its inputs, it learns to predict the changing input stream, and it learns to play back sequences of motor commands.
		</p>

		<p>HTM learns patterns that are changing over time.  Every neuron in the neocortex is learning transitions of patterns.  HTM is based on the memory and recall of a sequence of patterns.</p>
	</div>

	<input class="toggle-box" id="p2" type="checkbox">
	<label for="p2"><h3>Memory</h3></label>
	<div>
		<p>
		The neocortex is a memory system.  The neocortex must learn the structure of the world from the sensory patterns that stream into the brain. Each neuron learns by forming connections, and each region of the neocortex 
		is best understood as a type of memory system.
		</p>
	</div>

	<input class="toggle-box" id="p3" type="checkbox">
	<label for="p3"><h3>Sparse Distributed Representations (SDRs)</h3></label>
	<div>
		<p>
		A Sparse Distributed Representation is a binary data structure(0s or 1s) where a small percentage, typically 2% in HTM theory, of bits are 1.  Two SDRs that have 1 bits in the same location share a semantic property. 
		The more 1 bits two SDRs share, the more semantically similar are the two representations. <sup><a href="#1">[1]</a></sup>
		</p>

		[GRAPHIC]

		<p>Mathematical properties and advantages of SDRs include: <sup><a href="#1">[1]</a></sup></p>

		<ul>
			<li><b>Capacity of SDRs and the probability of mismatches</b> - </li>
			<li><b>Robustness of SDRs and the probability of error with noise</b> - </li>
			<li><b>Reliable classification of a list of SDR vectors</b> - </li>
			<li><b>Unions of SDRs</b> - </li>
			<li><b>Robustness of unions in the presence of noise</b> - </li>
		</ul>
	</div>

	<input class="toggle-box" id="p4" type="checkbox">
	<label for="p4"><h3>Sensory Encoders</h3></label>
	<div>
		WIP 
		Converts some type of data into an SDR to be processed by HTM algorithms.
	</div>

	<input class="toggle-box" id="p5" type="checkbox">
	<label for="p5"><h3>Sensory-Motor Systems</h3></label>
	<div>
		WIP

		We can't build systems that see and hear like humans do without incorporating movement of the eyes, body, and limbs.

		The neocortex learns a sensory-motor model of the world.  It learns given a sequence of sensory patterns and a sequence of actions what's going to happen next (prediction).
	</div>

	<input class="toggle-box" id="p6" type="checkbox">
	<label for="p6"><h3>On-line Unsupervised Learning</h3></label>
	<div>
		WIP 
		Strengthening and weakening of synapses 
	</div>

	<h2>Architecture</h2>

	<input class="toggle-box" id="ar0" type="checkbox">
	<label for="ar0"><h3>Regions</h3></label>
	<div>
		WIP

		<p>
		A HTM region is the equivalant of a cortical macrocolumn in a brain area (like V1 or MT), a collection of cortical minicolumns.  The neocortex typically contains 6 layers which HTM theory functionally
		</p>
		Layers

		<h4>Terms</h4>

		feedforward - inputs coming from lower regions
		feedback - inputs coming from higher regions
		lateral - inputs coming from regions of the same level
	</div>

	<input class="toggle-box" id="ar1" type="checkbox">
	<label for="ar1"><h3>Columns</h3></label>
	<div>
		WIP

		<p>
		A HTM column is the equivalent of a cortical minicolumn, a collection of neurons that respond to the same receptive field.
		</p>

		<h4>Terms</h4>
		
		Overlap
		
	</div>

	<input class="toggle-box" id="ar2" type="checkbox">
	<label for="ar2"><h3>Cells</h3></label>
	<div>
		<p>
		The structure of an HTM cell, or neuron, is modeled after pyramidal neurons in the neocortex. In a human neocortex, only about 2% of neurons are in an active state at any point in time (sparsity).  When a neuron is 
		active the axon outputs a short burst of action potentials. When it is in a predictive state the axon outputs a slower, steady rate of action potentials.  The action potentials, spikes with frequency, are not modeled 
		in HTM.
		</p>

		<img src="/webpages/technology/htm/cell.png" alt="cell" style="margin: 25px auto">

		<h4>Terms</h4>
		<ul>
			<li><b>axon</b> - In HTM, axons are modeled simply by using the cell state at the current time step.</li>
			<li><b>inactive</b> - In HTM, an inactive cell 
			<li><b>active</b> - In HTM, an active cell represents a recognized pattern in the context of active cells at the previous time step.  In neuroscience, an active cell is a neuron outputting action potentials.</li>
			<li><b>predict</b> - In HTM, a predict cell represents a potential occourance of a pattern at the next timestep in the context of active cells at the current timestep.  In neuroscience, a predict cell is a depolarized neuron which means it is more susceptible to feedforward inputs causeing the neuron to output action potentials.</li>
			<li><b>winner</b> - In HTM, a winner (or learn) cell is affected by the synaptic learning rules at the previous timestep.  There is no equivalent in neuroscience and is merely a computational optimization of HTM algorithms.</li>
		</ul>		
	</div>

	<input class="toggle-box" id="ar3" type="checkbox">
	<label for="ar3"><h3>Dendrite Segments</h3></label>
	<div>
		<p>
		It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. <sup><a href="#2">[2]</a></sup>  If a number of synapses become active at 
		relatively the same time (few milliseconds) in a short distance from each other (~40 microns) you get a non-linear event that generates a dendritic spike which puts the cell body into an anticipatory state.  Therefore,
		every little section of the dendritic tree is like a coincidence or threshold detector.
		</p>

		[GRAPHIC]
		
		<p>
		"Each dendrite segment has synapses.  For a neuron to recognize a pattern of activity it requires a set of co-located synapses (typically 15-20) that connect to a subset of the cells that are active in the pattern to be
		recognized. Learning to recognize a new pattern is accomplished by the formation of a set of new synapses collocated on a dendritic segment." <sup><a href="#2">[2]</a></sup>
		</p>

		<h4>Terms</h4>
		<ul>
			<li><b>proximal</b> - feedforward input affecting whether a column enters active state.  Used for pattern learning.</li>
			<li><b>distal</b> - lateral contextual input affecting whether a cell enters predict state.  Used for sequence learning.</li>
			<li><b>basal</b> - another name for distal sometimes used in HTM theory.</li>
			<li><b>apical</b> - feedback contextual input affecting whether a cell enters predict state.</li>
		</ul>
	</div>

	<input class="toggle-box" id="ar4" type="checkbox">
	<label for="ar4"><h3>Synapses</h3></label>
	<div>
		<p>
		Synapses form the fundamental memory storage of the neocortex.  HTM synapses use Hebbian-like learning rules to aquire knowledge about the world.  Over time synapses "grow" towards cells that are often active and 
		"shrink" away from cells that are often inactive.  This is modeled by a scalar value called permanence, floating-point values from 0.0 to 1.0 with a threshold of 0.3.  Note, the graphics display integer values from 0 to
		99 with a threshold of 30 purely for aesthetic purposes.  The synaptic learning rules are the same regardless of the data type.  Synapses behave in 5 ways:
		</p>
				
		<ul>
			<li><b>Grow</b> - If the post-synaptic cell is active, then increment the synapse permanence by the learning rate.</li>
			<li><b>Shrink</b> - If the post-synaptic cell is inactive, then decrement the synapse permanence by the learning rate.</li>
			<li><b>Adapt</b> - If permanence is 0, then move synapse to an unused post-synaptic cell and set the permanence just below the threshold.</li>
			<li><b>Insert</b> - If there are more active post-synaptic cells than synapses, then insert a synapse to an active post-synaptic cell.</li>
			<li><b>Remove</b> - If the synapse permanence is 0 and there are no available active post-synaptic cells, then remove the synapse.</li>
		</ul>

		<img src="/webpages/technology/htm/synapse.gif" alt="synapse" style="margin: 25px auto">
			
		<h4>Terms</h4>
		<ul>
			<li><b>pre-synaptic</b> - refering to where a synapse grows from, specifically the neuron dendrites in biology.</li>
			<li><b>post-synaptic</b> - refering to where a synapse grows towards, specifically the neuron axons in biology.</li>
			<li><b>permanence</b> - how connected a synapse is.</li>
			<li><b>threshold</b> - represents the establishment of a synapse.</li>
			<li><b>connected</b> - a synapse that has its permanence equal to or greater than the threshold value and therefore is affected by post-synaptic cell active or inactive states.</li>
			<li><b>unconnected</b> - a synapse that has its permanence less than the threshold value and therefore is not affected by post-synaptic cell active or inactive states.</li>
		</ul>
	</div>

	<h2>Algorithms</h2>

	<input class="toggle-box" id="al0" type="checkbox">
	<label for="al0"><h3>Initialization</h3></label>
	<div>
		<p>
		In HTM, each column sees a "potential pool", or random subset, of the input bits.  Each column's proximal dendrite segment may initialize its synapses randomly or arranged within a receptive field.  Synapse permanences 
		are initialized on or slightly below the threshold.
		</p>

		<img src="/webpages/technology/htm/syn-initialization.png" alt="syn-initialization" style="margin: 25px auto">		
	</div>

	<input class="toggle-box" id="al1" type="checkbox">
	<label for="al1"><h3>Spatial Pooling</h3></label>
	<div>
		<p>The Spatial Pooling algorithm selects a set of columns that best represent the input SDR at a single time step.  This is how an HTM region processes patterns, concurrent cell activations.</p>

		<img src="/webpages/technology/htm/sp.gif" alt="sp" style="margin: 25px auto">

		<h4>Overlap</h4>

		<p>
		For each column, compute the overlap score, or how how well the column's proximal synapses respond to the input SDR.  For each synapse in the column's proximal dendrite segment, if the synapse is connected and the 
		post-synaptic cell is active, increment the column's overlap.  Otherwise there is no effect on the column's overlap.
		</p>

		<h4>Inhibition</h4>

		<p>
		After each column computes its own overlap value, enforce sparsity by selecting a specified number of columns with the highest overlap score.  Usually ~2% of the possible columns will be activated.  Other columns may 
		represent the input pattern quite well, however they are being inhibited by columns with a stronger match.
		</p>

		<h4>Learning</h4>

		<p>Finally, adjust the column's proximal synapses to better match the input SDR by using the synaptic learning rules found in the previous section.</p>
	</div>

	<input class="toggle-box" id="al2" type="checkbox">
	<label for="al2"><h3>Temporal Memory</h3></label>
	<div>
		<p>The Temporal Memory algorithm uses previous cell states to set current cell states.  This is how an HTM region processes sequences, temporal cell activations.</p>

		<img src="/webpages/technology/htm/tm.gif" alt="tm" style="margin: 25px auto">

		<h4>Activation</h4>

		<p>If a cell in an active column at the current timestep is in the predict state at the previous timestep, then set the cell to the active state.</p>

		<h4>Bursting</h4>

		<p>If no cells in an active column at the current timestep are in the predict state at the previous timestep, then set all cells in the active column to the active state.</p>

		<h4>Prediction</h4>

		<h4>Learning</h4>
		
		<p>
		Winner cells at the current time step look for winner cells at the previous time step and forms connections to them on one of the dendritic segments.  Therefore, if the cell sees that pattern again the cell will 
		predict its own activity.
		</p>
	</div>

	<h2>References</h2>

	<ol>
		<li><a name="1" href="http://numenta.com/biological-and-machine-intelligence/">Jeff Hawkins et al. Biological and Machine Intelligence, 2016.</a></li>
		<li><a name="2" href="http://journal.frontiersin.org/article/10.3389/fncir.2016.00023/full">Jeff Hawkins and Subutai Ahmad. Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex, 2016.</a></li>
		<li><a name="3" href="https://github.com/numenta/nupic.research/blob/master/docs/bibliography/htm_bibliography.pdf">Subutai Ahmad and Yuwei Chu. Annotated Bibliography for HTM Researchers, 2016.</a></li>
		<li><a name="4" href="https://arxiv.org/abs/1512.05463">Yuwei Cui, Subutai Ahmad and Jeff Hawkins. Continuous Online Sequence Learning with an Unsupervised Neural Network Model, 2015.</a></li>
		<li><a name="5" href="https://discourse.numenta.org/t/preliminary-details-about-new-theory-work-on-sensory-motor-inference/697">Discourse: Preliminary details about new theory work on sensory-motor inference</a></li>
	</ol>
	
	<p>Page update: 2017-06-08</p>
</div>
</body>
</html>